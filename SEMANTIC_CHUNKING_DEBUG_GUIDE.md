# è¯­ä¹‰åˆ†å—è°ƒè¯•æŒ‡å—

## ğŸ“ å…¥å£ç‚¹

### API ç«¯ç‚¹
```
POST /console/api/datasets/indexing-estimate
```

**Controller æ–‡ä»¶**: `controllers/console/datasets/datasets.py`
**ç±»**: `DatasetIndexingEstimateApi`
**è¡Œæ•°**: 540-647

### è¯·æ±‚ç¤ºä¾‹
```json
{
  "info_list": {
    "data_source_type": "upload_file",
    "file_info_list": {
      "file_ids": ["0c3e9cc1-e7a0-4069-8d4c-eba4d169147e"]
    }
  },
  "indexing_technique": "high_quality",
  "process_rule": {
    "mode": "custom",
    "rules": {
      "pre_processing_rules": [
        { "id": "remove_extra_spaces", "enabled": true },
        { "id": "remove_urls_emails", "enabled": false }
      ],
      "segmentation": {
        "separator": "\\n\\n",
        "max_tokens": 1024,
        "chunk_overlap": 50,
        "threshold_amount": 95,
        "buffer_size": 2,
        "min_chunk_tokens": 150,
        "max_chunk_tokens": 1000
      }
    }
  },
  "doc_form": "semantic_model",
  "doc_language": "Chinese Simplified",
  "embedding_model": "emb",
  "embedding_model_provider": "langgenius/openai_api_compatible/openai_api_compatible"
}
```

---

## ğŸ”„ å®Œæ•´è°ƒç”¨æµç¨‹

### é˜¶æ®µ 1: API è¯·æ±‚å¤„ç†
**æ–‡ä»¶**: `controllers/console/datasets/datasets.py:540-647`

```python
def post(self):
    # 1. è§£æå‚æ•°
    parser = reqparse.RequestParser()
    args = parser.parse_args()

    # 2. å‚æ•°éªŒè¯
    DocumentService.estimate_args_validate(args)
    # ä½ç½®: services/dataset_service.py:2372-2452
    # éªŒè¯å†…å®¹: info_list, process_rule, segmentation å‚æ•°
    # åŒ…æ‹¬è¯­ä¹‰åˆ†å—å‚æ•°: threshold_amount, buffer_size, min_chunk_tokens, max_chunk_tokens

    # 3. è°ƒç”¨ indexing_estimate
    response = DocumentService.estimate(args)
```

**è°ƒè¯•ç‚¹ 1.1**: åœ¨ `datasets.py:560` æ‰“å°æ¥æ”¶åˆ°çš„ args
```python
print("=== DEBUG 1.1: Received args ===")
print(f"doc_form: {args.get('doc_form')}")
print(f"segmentation: {args.get('process_rule', {}).get('rules', {}).get('segmentation')}")
```

---

### é˜¶æ®µ 2: ä¼°ç®—æœåŠ¡å…¥å£
**æ–‡ä»¶**: `services/dataset_service.py`
**æ–¹æ³•**: `DocumentService.estimate()`
**è¡Œæ•°**: çº¦ 2100-2200

```python
@staticmethod
def estimate(args: dict) -> dict:
    # 1. æ„å»º extract_setting
    extract_setting = ExtractSetting(...)

    # 2. è°ƒç”¨ IndexingRunner.indexing_estimate
    indexing_estimate = IndexingRunner.indexing_estimate(
        tenant_id=current_user.current_tenant_id,
        extract_settings=[extract_setting],
        tmp_processing_rule=args["process_rule"],
        doc_form=args["doc_form"],  # "semantic_model"
        doc_language=args.get("doc_language", "English"),
        indexing_technique=args["indexing_technique"],
    )
```

**è°ƒè¯•ç‚¹ 2.1**: åœ¨ `DocumentService.estimate()` æ–¹æ³•å¼€å§‹å¤„
```python
print("=== DEBUG 2.1: DocumentService.estimate ===")
print(f"doc_form: {args['doc_form']}")
print(f"indexing_technique: {args['indexing_technique']}")
print(f"process_rule mode: {args['process_rule']['mode']}")
```

---

### é˜¶æ®µ 3: ç´¢å¼•è¿è¡Œå™¨ä¼°ç®—
**æ–‡ä»¶**: `core/indexing_runner.py`
**æ–¹æ³•**: `IndexingRunner.indexing_estimate()`
**è¡Œæ•°**: 245-343

```python
@classmethod
def indexing_estimate(
    cls,
    tenant_id: str,
    extract_settings: list[ExtractSetting],
    tmp_processing_rule: dict,
    doc_form: str = "text_model",  # è¿™é‡Œä¼šä¼ å…¥ "semantic_model"
    doc_language: str = "English",
    dataset_id: Optional[str] = None,
    indexing_technique: str = "economy",
) -> IndexingEstimate:

    # 1. åˆ›å»º IndexProcessor
    index_processor = IndexProcessorFactory(doc_form).init_index_processor()
    # å½“ doc_form = "semantic_model" æ—¶ï¼Œåˆ›å»º SemanticIndexProcessor

    # 2. Extract é˜¶æ®µ - æå–æ–‡æ¡£å†…å®¹
    documents = index_processor.extract(extract_setting, ...)

    # 3. Transform é˜¶æ®µ - è¯­ä¹‰åˆ†å—
    documents = index_processor.transform(
        documents=documents,
        process_rule=tmp_processing_rule,
        embedding_model_instance=embedding_model_instance,
        ...
    )

    # 4. è¿”å›ç»“æœ
    return IndexingEstimate(
        total_segments=len(documents),
        preview=documents[:10],
        ...
    )
```

**è°ƒè¯•ç‚¹ 3.1**: åœ¨åˆ›å»º processor å
```python
print("=== DEBUG 3.1: IndexProcessor Created ===")
print(f"Processor type: {type(index_processor).__name__}")
print(f"doc_form: {doc_form}")
```

**è°ƒè¯•ç‚¹ 3.2**: Extract é˜¶æ®µå
```python
print("=== DEBUG 3.2: After Extract ===")
print(f"Number of documents: {len(documents)}")
for i, doc in enumerate(documents[:3]):
    print(f"Doc {i} length: {len(doc.page_content)} chars")
    print(f"Doc {i} preview: {doc.page_content[:100]}...")
```

**è°ƒè¯•ç‚¹ 3.3**: Transform é˜¶æ®µå
```python
print("=== DEBUG 3.3: After Transform (Semantic Chunking) ===")
print(f"Number of chunks: {len(documents)}")
for i, doc in enumerate(documents[:5]):
    print(f"Chunk {i} length: {len(doc.page_content)} chars")
    print(f"Chunk {i} preview: {doc.page_content[:80]}...")
```

---

### é˜¶æ®µ 4: è¯­ä¹‰ç´¢å¼•å¤„ç†å™¨
**æ–‡ä»¶**: `core/rag/index_processor/processor/semantic_index_processor.py`

#### 4.1 Extract æ–¹æ³•
**è¡Œæ•°**: 45-59

```python
def extract(self, extract_setting: ExtractSetting, **kwargs) -> list[Document]:
    text_docs = ExtractProcessor.extract(
        extract_setting=extract_setting,
        is_automatic=(kwargs.get("process_rule_mode") == "automatic" or ...),
    )
    return text_docs
```

**è°ƒè¯•ç‚¹ 4.1**: Extract å¼€å§‹å’Œç»“æŸ
```python
print("=== DEBUG 4.1: SemanticIndexProcessor.extract START ===")
print(f"extract_setting: {extract_setting}")

# ... extraction logic ...

print("=== DEBUG 4.1: SemanticIndexProcessor.extract END ===")
print(f"Extracted {len(text_docs)} documents")
for i, doc in enumerate(text_docs):
    print(f"  Doc {i}: {len(doc.page_content)} chars")
```

#### 4.2 Transform æ–¹æ³• (æ ¸å¿ƒè¯­ä¹‰åˆ†å—)
**è¡Œæ•°**: 61-127

```python
def transform(self, documents: list[Document], **kwargs) -> list[Document]:
    # 1. è·å– process_rule
    process_rule = kwargs.get("process_rule")
    rules = Rule.model_validate(process_rule.get("rules"))

    # 2. åˆ›å»º SemanticTextSplitter
    splitter = SemanticTextSplitter(
        separator=rules.segmentation.separator,
        max_tokens=rules.segmentation.max_tokens,
        chunk_overlap=rules.segmentation.chunk_overlap,
        threshold_amount=rules.segmentation.threshold_amount or 95,
        buffer_size=rules.segmentation.buffer_size or 2,
        min_chunk_tokens=rules.segmentation.min_chunk_tokens or 150,
        max_chunk_tokens=rules.segmentation.max_chunk_tokens or rules.segmentation.max_tokens,
        embedding_model_instance=embedding_model_instance,
    )

    # 3. å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œå¤„ç†
    for document in documents:
        # 3.1 æ¸…ç†æ–‡æ¡£
        document_text = CleanProcessor.clean(document.page_content, ...)

        # 3.2 è¯­ä¹‰åˆ†å—
        document_nodes = splitter.split_documents([document])

        # 3.3 åå¤„ç†
        for document_node in document_nodes:
            # æ·»åŠ  metadata, hash ç­‰
            ...

    return all_documents
```

**è°ƒè¯•ç‚¹ 4.2**: Transform å„é˜¶æ®µ
```python
print("=== DEBUG 4.2: SemanticIndexProcessor.transform START ===")
print(f"Input documents: {len(documents)}")
print(f"Segmentation config:")
print(f"  separator: {rules.segmentation.separator}")
print(f"  max_tokens: {rules.segmentation.max_tokens}")
print(f"  chunk_overlap: {rules.segmentation.chunk_overlap}")
print(f"  threshold_amount: {rules.segmentation.threshold_amount}")
print(f"  buffer_size: {rules.segmentation.buffer_size}")
print(f"  min_chunk_tokens: {rules.segmentation.min_chunk_tokens}")
print(f"  max_chunk_tokens: {rules.segmentation.max_chunk_tokens}")

# åœ¨å¾ªç¯ä¸­
for idx, document in enumerate(documents):
    print(f"\n--- Processing document {idx} ---")
    print(f"Original length: {len(document.page_content)} chars")

    # æ¸…ç†å
    print(f"After cleaning: {len(document_text)} chars")

    # åˆ†å—å
    print(f"Generated {len(document_nodes)} chunks")
    for i, node in enumerate(document_nodes[:3]):
        print(f"  Chunk {i}: {len(node.page_content)} chars")

print("=== DEBUG 4.2: SemanticIndexProcessor.transform END ===")
print(f"Total output chunks: {len(all_documents)}")
```

---

### é˜¶æ®µ 5: è¯­ä¹‰æ–‡æœ¬åˆ†å‰²å™¨ (æ ¸å¿ƒç®—æ³•)
**æ–‡ä»¶**: `core/rag/splitter/semantic_text_splitter.py`

#### 5.1 ä¸»å…¥å£: split_text
**è¡Œæ•°**: 69-96

```python
def split_text(self, text: str) -> list[str]:
    # Step 1: æŒ‰ separator åˆ‡åˆ†ç‰©ç†è¾¹ç•Œ
    paragraphs = self._split_by_separator(text)

    # Step 2: åˆ‡åˆ†æˆå¥å­
    all_sentences = []
    for paragraph in paragraphs:
        sentences = self._split_into_sentences(paragraph)
        all_sentences.extend(sentences)

    # Step 3-4: ç”Ÿæˆ embeddings å¹¶æ‰¾è¯­ä¹‰è¾¹ç•Œ
    semantic_boundaries = self._find_semantic_boundaries(all_sentences)

    # Step 5: ç”Ÿæˆè¯­ä¹‰å—
    semantic_chunks = self._generate_semantic_chunks(all_sentences, semantic_boundaries)

    # Step 6: åå¤„ç† (åˆå¹¶çŸ­å—ã€åˆ‡åˆ†é•¿å—ã€æ·»åŠ é‡å )
    final_chunks = self._post_process_chunks(semantic_chunks)

    return final_chunks
```

**è°ƒè¯•ç‚¹ 5.1**: split_text ä¸»æµç¨‹
```python
print("=== DEBUG 5.1: SemanticTextSplitter.split_text START ===")
print(f"Input text length: {len(text)} chars")

# Step 1
print(f"\nStep 1: Split by separator")
print(f"Paragraphs: {len(paragraphs)}")
for i, para in enumerate(paragraphs[:3]):
    print(f"  Para {i}: {len(para)} chars")

# Step 2
print(f"\nStep 2: Split into sentences")
print(f"Total sentences: {len(all_sentences)}")
for i, sent in enumerate(all_sentences[:5]):
    print(f"  Sent {i}: {sent[:60]}...")

# Step 3-4
print(f"\nStep 3-4: Find semantic boundaries")
print(f"Boundaries found: {semantic_boundaries}")

# Step 5
print(f"\nStep 5: Generate semantic chunks")
print(f"Semantic chunks: {len(semantic_chunks)}")
for i, chunk in enumerate(semantic_chunks[:3]):
    print(f"  Chunk {i}: {len(chunk)} chars")

# Step 6
print(f"\nStep 6: Post-process chunks")
print(f"Final chunks: {len(final_chunks)}")
for i, chunk in enumerate(final_chunks[:3]):
    print(f"  Chunk {i}: {len(chunk)} chars, {self._get_token_count(chunk)} tokens")

print("=== DEBUG 5.1: SemanticTextSplitter.split_text END ===")
```

#### 5.2 æŒ‰åˆ†éš”ç¬¦åˆ‡åˆ†
**è¡Œæ•°**: 98-103

```python
def _split_by_separator(self, text: str) -> list[str]:
    if self._separator:
        parts = text.split(self._separator)
        return [p.strip() for p in parts if p.strip()]
    return [text]
```

**è°ƒè¯•ç‚¹ 5.2**:
```python
print(f"=== DEBUG 5.2: _split_by_separator ===")
print(f"Separator: {repr(self._separator)}")
print(f"Input length: {len(text)}")
parts = text.split(self._separator) if self._separator else [text]
print(f"Raw parts: {len(parts)}")
result = [p.strip() for p in parts if p.strip()]
print(f"Cleaned parts: {len(result)}")
return result
```

#### 5.3 åˆ‡åˆ†å¥å­
**è¡Œæ•°**: 105-158

```python
def _split_into_sentences(self, text: str) -> list[str]:
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†
    # æ”¯æŒä¸­æ–‡ (ã€‚ï¼ï¼Ÿ) å’Œè‹±æ–‡ (.!?\s+)
    combined_pattern = '|'.join(f'({p})' for p in self._sentence_patterns)
    parts = re.split(combined_pattern, text)

    # é‡æ–°ç»„è£…å¥å­ï¼ˆåŒ…å«åˆ†éš”ç¬¦ï¼‰
    sentences = []
    current_sentence = ""
    for part in parts:
        # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†éš”ç¬¦
        # æ‹¼æ¥å¥å­
        ...

    return sentences
```

**è°ƒè¯•ç‚¹ 5.3**:
```python
print(f"=== DEBUG 5.3: _split_into_sentences ===")
print(f"Input: {text[:100]}...")
print(f"Patterns: {self._sentence_patterns}")

# åœ¨ split å
print(f"Raw parts: {len(parts)}")

# åœ¨å¾ªç¯ä¸­
for i, part in enumerate(parts[:10]):
    print(f"  Part {i}: {repr(part[:30])}")

# ç»“æœ
print(f"Sentences found: {len(sentences)}")
for i, sent in enumerate(sentences[:5]):
    print(f"  Sent {i}: {sent[:60]}...")
```

#### 5.4 æŸ¥æ‰¾è¯­ä¹‰è¾¹ç•Œ (æ ¸å¿ƒç®—æ³•)
**è¡Œæ•°**: 178-220

```python
def _find_semantic_boundaries(self, sentences: list[str]) -> list[int]:
    # 1. ç”Ÿæˆ embeddings
    embeddings = self._get_embeddings(sentences)

    # 2. è®¡ç®—ç›¸é‚»å¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = []
    for i in range(len(embeddings) - 1):
        sim = self._cosine_similarity(embeddings[i], embeddings[i + 1])
        similarities.append(sim)

    # 3. åº”ç”¨å¹³æ»‘ (buffer_size)
    smoothed_similarities = self._apply_smoothing(similarities, self._buffer_size)

    # 4. ä½¿ç”¨ç™¾åˆ†ä½æ•°è®¡ç®—é˜ˆå€¼
    threshold = np.percentile(smoothed_similarities, self._threshold_amount)

    # 5. æ‰¾å‡ºä½äºé˜ˆå€¼çš„ä½ç½®ä½œä¸ºè¾¹ç•Œ
    boundaries = []
    for i, sim in enumerate(smoothed_similarities):
        if sim < threshold:
            boundaries.append(i + 1)

    return boundaries
```

**è°ƒè¯•ç‚¹ 5.4**: è¯­ä¹‰è¾¹ç•Œæ£€æµ‹è¯¦ç»†è¿‡ç¨‹
```python
print(f"=== DEBUG 5.4: _find_semantic_boundaries ===")
print(f"Input sentences: {len(sentences)}")

# 1. Embeddings
print(f"\nStep 1: Generate embeddings")
embeddings = self._get_embeddings(sentences)
print(f"Embeddings shape: {embeddings.shape}")

# 2. Similarities
print(f"\nStep 2: Calculate similarities")
similarities = []
for i in range(len(embeddings) - 1):
    sim = self._cosine_similarity(embeddings[i], embeddings[i + 1])
    similarities.append(sim)
    if i < 5:
        print(f"  Sim[{i}â†’{i+1}]: {sim:.4f}")
print(f"Similarities: min={min(similarities):.4f}, max={max(similarities):.4f}, mean={np.mean(similarities):.4f}")

# 3. Smoothing
print(f"\nStep 3: Apply smoothing (buffer_size={self._buffer_size})")
smoothed_similarities = self._apply_smoothing(similarities, self._buffer_size)
print(f"Smoothed: min={min(smoothed_similarities):.4f}, max={max(smoothed_similarities):.4f}")

# 4. Threshold
print(f"\nStep 4: Calculate threshold (percentile={self._threshold_amount})")
threshold = np.percentile(smoothed_similarities, self._threshold_amount)
print(f"Threshold: {threshold:.4f}")

# 5. Boundaries
print(f"\nStep 5: Find boundaries")
boundaries = []
for i, sim in enumerate(smoothed_similarities):
    if sim < threshold:
        boundaries.append(i + 1)
        if len(boundaries) <= 5:
            print(f"  Boundary at position {i+1}, sim={sim:.4f}")
print(f"Total boundaries: {len(boundaries)}")
print(f"Boundaries: {boundaries[:10]}...")

return boundaries
```

#### 5.5 ç”Ÿæˆ Embeddings
**è¡Œæ•°**: 172-176

```python
def _get_embeddings(self, texts: list[str]) -> np.ndarray:
    if self._embedding_model_instance:
        # ä½¿ç”¨ embedding æ¨¡å‹
        embeddings = self._embedding_model_instance.invoke_text_embedding(texts=texts)
        return np.array(embeddings)
    else:
        # ä½¿ç”¨åå¤‡æ–¹æ¡ˆ
        return self._fallback_embeddings(texts)
```

**è°ƒè¯•ç‚¹ 5.5**:
```python
print(f"=== DEBUG 5.5: _get_embeddings ===")
print(f"Texts to embed: {len(texts)}")
print(f"Has embedding model: {self._embedding_model_instance is not None}")

if self._embedding_model_instance:
    print(f"Using embedding model")
    embeddings = self._embedding_model_instance.invoke_text_embedding(texts=texts)
    embeddings = np.array(embeddings)
    print(f"Embeddings shape: {embeddings.shape}")
else:
    print(f"Using fallback embeddings")
    embeddings = self._fallback_embeddings(texts)
    print(f"Fallback embeddings shape: {embeddings.shape}")

return embeddings
```

#### 5.6 å¹³æ»‘å¤„ç†
**è¡Œæ•°**: 255-272

```python
def _apply_smoothing(self, similarities: list[float], buffer_size: int) -> list[float]:
    smoothed = []
    for i in range(len(similarities)):
        start = max(0, i - buffer_size)
        end = min(len(similarities), i + buffer_size + 1)
        window = similarities[start:end]
        smoothed.append(sum(window) / len(window))
    return smoothed
```

**è°ƒè¯•ç‚¹ 5.6**:
```python
print(f"=== DEBUG 5.6: _apply_smoothing ===")
print(f"Input similarities: {len(similarities)}")
print(f"Buffer size: {buffer_size}")

smoothed = []
for i in range(len(similarities)):
    start = max(0, i - buffer_size)
    end = min(len(similarities), i + buffer_size + 1)
    window = similarities[start:end]
    avg = sum(window) / len(window)
    smoothed.append(avg)
    if i < 5:
        print(f"  Position {i}: window[{start}:{end}], original={similarities[i]:.4f}, smoothed={avg:.4f}")

print(f"Smoothed: {len(smoothed)} values")
return smoothed
```

#### 5.7 ç”Ÿæˆè¯­ä¹‰å—
**è¡Œæ•°**: 274-297

```python
def _generate_semantic_chunks(self, sentences: list[str], boundaries: list[int]) -> list[str]:
    chunks = []
    start_idx = 0

    for boundary_idx in boundaries:
        if boundary_idx > start_idx:
            chunk_sentences = sentences[start_idx:boundary_idx]
            chunk_text = ' '.join(chunk_sentences)
            chunks.append(chunk_text)
            start_idx = boundary_idx

    # æ·»åŠ å‰©ä½™å¥å­
    if start_idx < len(sentences):
        chunk_sentences = sentences[start_idx:]
        chunk_text = ' '.join(chunk_sentences)
        chunks.append(chunk_text)

    return chunks
```

**è°ƒè¯•ç‚¹ 5.7**:
```python
print(f"=== DEBUG 5.7: _generate_semantic_chunks ===")
print(f"Total sentences: {len(sentences)}")
print(f"Boundaries: {boundaries}")

chunks = []
start_idx = 0

for idx, boundary_idx in enumerate(boundaries):
    if boundary_idx > start_idx:
        chunk_sentences = sentences[start_idx:boundary_idx]
        chunk_text = ' '.join(chunk_sentences)
        chunks.append(chunk_text)
        print(f"  Chunk {idx}: sentences[{start_idx}:{boundary_idx}] = {len(chunk_sentences)} sentences, {len(chunk_text)} chars")
        start_idx = boundary_idx

if start_idx < len(sentences):
    chunk_sentences = sentences[start_idx:]
    chunk_text = ' '.join(chunk_sentences)
    chunks.append(chunk_text)
    print(f"  Final chunk: sentences[{start_idx}:] = {len(chunk_sentences)} sentences, {len(chunk_text)} chars")

print(f"Total chunks generated: {len(chunks)}")
return chunks
```

#### 5.8 åå¤„ç† (åˆå¹¶ã€åˆ‡åˆ†ã€é‡å )
**è¡Œæ•°**: 299-319

```python
def _post_process_chunks(self, chunks: list[str]) -> list[str]:
    # Step 1: åˆå¹¶çŸ­å— (< min_chunk_tokens)
    merged_chunks = self._merge_short_chunks(chunks)

    # Step 2: åˆ‡åˆ†é•¿å— (> max_chunk_tokens)
    split_chunks = self._split_long_chunks(merged_chunks)

    # Step 3: æ·»åŠ é‡å 
    final_chunks = self._add_overlap(split_chunks)

    return final_chunks
```

**è°ƒè¯•ç‚¹ 5.8**:
```python
print(f"=== DEBUG 5.8: _post_process_chunks ===")
print(f"Input chunks: {len(chunks)}")

# Step 1
merged_chunks = self._merge_short_chunks(chunks)
print(f"\nStep 1: Merge short chunks (< {self._min_chunk_tokens} tokens)")
print(f"After merging: {len(merged_chunks)} chunks")
for i, chunk in enumerate(merged_chunks[:3]):
    tokens = self._get_token_count(chunk)
    print(f"  Chunk {i}: {len(chunk)} chars, {tokens} tokens")

# Step 2
split_chunks = self._split_long_chunks(merged_chunks)
print(f"\nStep 2: Split long chunks (> {self._max_chunk_tokens} tokens)")
print(f"After splitting: {len(split_chunks)} chunks")
for i, chunk in enumerate(split_chunks[:3]):
    tokens = self._get_token_count(chunk)
    print(f"  Chunk {i}: {len(chunk)} chars, {tokens} tokens")

# Step 3
final_chunks = self._add_overlap(split_chunks)
print(f"\nStep 3: Add overlap ({self._chunk_overlap} tokens)")
print(f"Final chunks: {len(final_chunks)} chunks")
for i, chunk in enumerate(final_chunks[:3]):
    tokens = self._get_token_count(chunk)
    print(f"  Chunk {i}: {len(chunk)} chars, {tokens} tokens")

return final_chunks
```

---

## ğŸ” å¿«é€Ÿè°ƒè¯•æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: åœ¨ API å±‚æ·»åŠ è¯¦ç»†æ—¥å¿—

åœ¨ `controllers/console/datasets/datasets.py` çš„ `DatasetIndexingEstimateApi.post()` æ–¹æ³•ä¸­ï¼š

```python
def post(self):
    import json

    parser = reqparse.RequestParser()
    # ... parse args ...

    # è°ƒè¯•ç‚¹: æ‰“å°å®Œæ•´è¯·æ±‚
    print("\n" + "="*80)
    print("SEMANTIC CHUNKING DEBUG - API Entry")
    print("="*80)
    print(json.dumps(args, indent=2, ensure_ascii=False))
    print("="*80 + "\n")

    # ... rest of method ...

    # è°ƒè¯•ç‚¹: æ‰“å°ç»“æœ
    print("\n" + "="*80)
    print("SEMANTIC CHUNKING DEBUG - API Result")
    print("="*80)
    print(f"Total segments: {response['total_segments']}")
    print(f"Preview count: {len(response.get('preview', []))}")
    for i, preview in enumerate(response.get('preview', [])[:3]):
        print(f"\nChunk {i}:")
        print(f"  Length: {len(preview.get('content', ''))} chars")
        print(f"  Content: {preview.get('content', '')[:100]}...")
    print("="*80 + "\n")

    return response
```

### æ–¹æ¡ˆ 2: åœ¨ SemanticTextSplitter æ·»åŠ æ—¥å¿—

åœ¨ `core/rag/splitter/semantic_text_splitter.py` çš„ `split_text()` æ–¹æ³•ä¸­ï¼š

```python
def split_text(self, text: str) -> list[str]:
    print("\n" + "="*80)
    print("SEMANTIC TEXT SPLITTER - START")
    print("="*80)
    print(f"Input: {len(text)} chars")
    print(f"Config:")
    print(f"  separator: {repr(self._separator)}")
    print(f"  max_tokens: {self._max_tokens}")
    print(f"  chunk_overlap: {self._chunk_overlap}")
    print(f"  threshold_amount: {self._threshold_amount}")
    print(f"  buffer_size: {self._buffer_size}")
    print(f"  min_chunk_tokens: {self._min_chunk_tokens}")
    print(f"  max_chunk_tokens: {self._max_chunk_tokens}")

    # Step 1
    paragraphs = self._split_by_separator(text)
    print(f"\n[Step 1] Paragraphs: {len(paragraphs)}")

    # Step 2
    all_sentences = []
    for para in paragraphs:
        sentences = self._split_into_sentences(para)
        all_sentences.extend(sentences)
    print(f"[Step 2] Sentences: {len(all_sentences)}")

    # Step 3-4
    semantic_boundaries = self._find_semantic_boundaries(all_sentences)
    print(f"[Step 3-4] Boundaries: {len(semantic_boundaries)}")
    print(f"  Positions: {semantic_boundaries[:10]}...")

    # Step 5
    semantic_chunks = self._generate_semantic_chunks(all_sentences, semantic_boundaries)
    print(f"[Step 5] Semantic chunks: {len(semantic_chunks)}")

    # Step 6
    final_chunks = self._post_process_chunks(semantic_chunks)
    print(f"[Step 6] Final chunks: {len(final_chunks)}")

    print("\n" + "="*80)
    print("SEMANTIC TEXT SPLITTER - END")
    print(f"Result: {len(final_chunks)} chunks")
    for i, chunk in enumerate(final_chunks[:3]):
        tokens = self._get_token_count(chunk)
        print(f"\nChunk {i}: {len(chunk)} chars, {tokens} tokens")
        print(f"Preview: {chunk[:80]}...")
    print("="*80 + "\n")

    return final_chunks
```

### æ–¹æ¡ˆ 3: åˆ›å»ºæµ‹è¯•è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `test_semantic_chunking.py`:

```python
import sys
sys.path.insert(0, '/home/user/dify_api')

from core.rag.splitter.semantic_text_splitter import SemanticTextSplitter

# æµ‹è¯•æ–‡æœ¬
test_text = """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„ç ”ç©¶ã€‚

æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸åŒ…å«å¤šä¸ªéšè—å±‚ã€‚æ¯ä¸€å±‚éƒ½å¯ä»¥å­¦ä¹ æ•°æ®çš„ä¸åŒç‰¹å¾ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ä»»åŠ¡ã€‚

è®¡ç®—æœºè§†è§‰æ˜¯æ·±åº¦å­¦ä¹ çš„é‡è¦åº”ç”¨é¢†åŸŸã€‚å·ç§¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒä»¬å¯ä»¥è‡ªåŠ¨å­¦ä¹ å›¾åƒçš„ç‰¹å¾ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†ä¹Ÿå—ç›Šäºæ·±åº¦å­¦ä¹ ã€‚å¾ªç¯ç¥ç»ç½‘ç»œå’ŒTransformeræ¨¡å‹åœ¨æ–‡æœ¬å¤„ç†ä¸­éå¸¸æœ‰æ•ˆã€‚å®ƒä»¬èƒ½å¤Ÿç†è§£è¯­è¨€çš„ä¸Šä¸‹æ–‡å…³ç³»ã€‚
"""

# åˆ›å»ºåˆ†å‰²å™¨
splitter = SemanticTextSplitter(
    separator="\n\n",
    max_tokens=1024,
    chunk_overlap=50,
    threshold_amount=95,
    buffer_size=2,
    min_chunk_tokens=150,
    max_chunk_tokens=1000,
    embedding_model_instance=None  # ä½¿ç”¨åå¤‡æ–¹æ¡ˆ
)

# æ‰§è¡Œåˆ†å—
chunks = splitter.split_text(test_text)

# æ‰“å°ç»“æœ
print(f"\nç”Ÿæˆäº† {len(chunks)} ä¸ªå—:\n")
for i, chunk in enumerate(chunks):
    print(f"--- å— {i+1} ---")
    print(f"é•¿åº¦: {len(chunk)} å­—ç¬¦")
    print(f"å†…å®¹: {chunk}")
    print()
```

è¿è¡Œæµ‹è¯•:
```bash
cd /home/user/dify_api
python3 test_semantic_chunking.py
```

---

## ğŸ“Š å…³é”®æŒ‡æ ‡ç›‘æ§

åœ¨è°ƒè¯•è¿‡ç¨‹ä¸­ï¼Œå…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š

### è¾“å…¥é˜¶æ®µ
- åŸå§‹æ–‡æ¡£é•¿åº¦
- åˆ†éš”ç¬¦ç±»å‹
- è¯­ä¹‰å‚æ•°é…ç½®

### å¤„ç†é˜¶æ®µ
- æ®µè½æ•°é‡
- å¥å­æ•°é‡
- ç›¸ä¼¼åº¦åˆ†å¸ƒ (min, max, mean)
- é˜ˆå€¼å¤§å°
- è¾¹ç•Œä½ç½®

### è¾“å‡ºé˜¶æ®µ
- å—æ•°é‡
- æ¯å—çš„ token æ•°
- æ˜¯å¦æœ‰è¿‡çŸ­/è¿‡é•¿çš„å—
- é‡å æ˜¯å¦æ­£ç¡®åº”ç”¨

---

## ğŸ› å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1: ç”Ÿæˆçš„å—å¤ªå¤šæˆ–å¤ªå°‘
**æ£€æŸ¥ç‚¹**:
- `threshold_amount` å€¼ (å»ºè®® 90-98)
- `buffer_size` å€¼ (å»ºè®® 1-5)
- å¥å­åˆ‡åˆ†æ˜¯å¦æ­£ç¡®

### é—®é¢˜ 2: å—å¤§å°ä¸ç¬¦åˆé¢„æœŸ
**æ£€æŸ¥ç‚¹**:
- `min_chunk_tokens` å’Œ `max_chunk_tokens` è®¾ç½®
- token è®¡æ•°å‡½æ•°æ˜¯å¦æ­£ç¡®
- `_merge_short_chunks` å’Œ `_split_long_chunks` é€»è¾‘

### é—®é¢˜ 3: è¯­ä¹‰è¾¹ç•Œä¸å‡†ç¡®
**æ£€æŸ¥ç‚¹**:
- Embedding æ¨¡å‹æ˜¯å¦å¯ç”¨
- ç›¸ä¼¼åº¦è®¡ç®—æ˜¯å¦æ­£ç¡®
- å¹³æ»‘å¤„ç†æ˜¯å¦åˆç†

### é—®é¢˜ 4: é‡å ä¸æ­£ç¡®
**æ£€æŸ¥ç‚¹**:
- `chunk_overlap` å€¼
- `_add_overlap` å®ç°é€»è¾‘

---

## ğŸ“ æ—¥å¿—è¾“å‡ºç¤ºä¾‹

å¯ç”¨æ‰€æœ‰è°ƒè¯•ç‚¹åï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š

```
=== DEBUG 1.1: Received args ===
doc_form: semantic_model
segmentation: {'separator': '\\n\\n', 'max_tokens': 1024, ...}

=== DEBUG 2.1: DocumentService.estimate ===
doc_form: semantic_model
indexing_technique: high_quality

=== DEBUG 3.1: IndexProcessor Created ===
Processor type: SemanticIndexProcessor
doc_form: semantic_model

=== DEBUG 3.2: After Extract ===
Number of documents: 1
Doc 0 length: 5234 chars

=== DEBUG 4.2: SemanticIndexProcessor.transform START ===
Input documents: 1
Segmentation config:
  separator: \n\n
  max_tokens: 1024
  threshold_amount: 95
  ...

=== DEBUG 5.1: SemanticTextSplitter.split_text START ===
Step 1: Split by separator
Paragraphs: 4

Step 2: Split into sentences
Total sentences: 12

Step 3-4: Find semantic boundaries
Boundaries found: [3, 7, 10]

Step 5: Generate semantic chunks
Semantic chunks: 4

Step 6: Post-process chunks
Final chunks: 5

=== DEBUG 5.4: _find_semantic_boundaries ===
Similarities: min=0.3421, max=0.9876, mean=0.7234
Threshold: 0.8234
Boundaries: [3, 7, 10]

=== DEBUG 3.3: After Transform ===
Number of chunks: 5
Chunk 0: 234 chars
Chunk 1: 456 chars
...
```

---

## ğŸ¯ å»ºè®®çš„è°ƒè¯•é¡ºåº

1. **å…ˆæµ‹è¯• API å…¥å£** - ç¡®è®¤è¯·æ±‚èƒ½æ­£ç¡®åˆ°è¾¾
2. **æ£€æŸ¥å‚æ•°éªŒè¯** - ç¡®ä¿å‚æ•°æ ¼å¼æ­£ç¡®
3. **éªŒè¯ Processor åˆ›å»º** - ç¡®è®¤ä½¿ç”¨äº† SemanticIndexProcessor
4. **è¿½è¸ª Extract** - ç¡®è®¤æ–‡æ¡£æ­£ç¡®æå–
5. **æ·±å…¥ Transform** - è¿™æ˜¯æ ¸å¿ƒï¼Œé‡ç‚¹è°ƒè¯•
6. **è¯¦æŸ¥ split_text** - é€æ­¥éªŒè¯æ¯ä¸ªé˜¶æ®µ
7. **åˆ†æè¾¹ç•Œæ£€æµ‹** - æ£€æŸ¥ç›¸ä¼¼åº¦å’Œé˜ˆå€¼
8. **éªŒè¯åå¤„ç†** - ç¡®ä¿åˆå¹¶/åˆ‡åˆ†/é‡å æ­£ç¡®

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
- å®Œæ•´çš„è¯·æ±‚ JSON
- å…³é”®è°ƒè¯•ç‚¹çš„è¾“å‡º
- æœŸæœ›çš„ç»“æœ vs å®é™…ç»“æœ
- é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
