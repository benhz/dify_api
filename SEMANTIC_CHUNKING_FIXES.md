# è¯­ä¹‰åˆ†å—å…³é”®ä¿®å¤è¯´æ˜

## ğŸ› å·²ä¿®å¤çš„ä¸¤ä¸ªè‡´å‘½é”™è¯¯

### é”™è¯¯ 1: Embedding çˆ†ç‚¸ âš ï¸

**é—®é¢˜æè¿°**ï¼š
åŸå®ç°ä¼šå°†æ‰€æœ‰æ®µè½çš„æ‰€æœ‰å¥å­ä¸€æ¬¡æ€§å‘é€ç»™ embedding æ¨¡å‹å¤„ç†ï¼Œå¯¼è‡´ï¼š
- è¯·æ±‚æ•°æ®é‡è¿‡å¤§
- Embedding æ¨¡å‹è¶…è½½
- å¤„ç†é€Ÿåº¦ææ…¢æˆ–å¤±è´¥

**åŸå§‹æµç¨‹ï¼ˆé”™è¯¯ï¼‰**ï¼š
```python
def split_text(self, text: str) -> list[str]:
    # Step 1: åˆ‡åˆ†æ‰€æœ‰æ®µè½
    paragraphs = self._split_by_separator(text)

    # Step 2: æå–æ‰€æœ‰æ®µè½çš„æ‰€æœ‰å¥å­
    all_sentences = []
    for paragraph in paragraphs:
        sentences = self._split_into_sentences(paragraph)
        all_sentences.extend(sentences)  # âŒ ç´¯ç§¯æ‰€æœ‰å¥å­

    # Step 3: ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å¥å­
    semantic_boundaries = self._find_semantic_boundaries(all_sentences)  # âŒ çˆ†ç‚¸ï¼
    # ... å¦‚æœæœ‰100ä¸ªæ®µè½ï¼Œæ¯ä¸ª10å¥è¯ï¼Œå°±æ˜¯1000å¥è¯ä¸€èµ·å¤„ç†
```

**ä¿®å¤åçš„æµç¨‹ï¼ˆæ­£ç¡®ï¼‰**ï¼š
```python
def split_text(self, text: str) -> list[str]:
    # Step 1: åˆ‡åˆ†æ®µè½
    paragraphs = self._split_by_separator(text)

    # Step 2: é€ä¸ªæ®µè½å¤„ç†
    all_chunks = []
    for paragraph in paragraphs:
        para_tokens = self._get_token_count(paragraph)

        if para_tokens <= self._max_tokens:
            # âœ“ å°æ®µè½ç›´æ¥ä¿ç•™ï¼Œä¸åšè¯­ä¹‰åˆ†æ
            all_chunks.append(paragraph)
        else:
            # âœ“ åªå¯¹å¤§æ®µè½åšè¯­ä¹‰åˆ†æ
            para_chunks = self._split_paragraph_semantically(paragraph)
            all_chunks.extend(para_chunks)

    # Step 3: åå¤„ç†
    final_chunks = self._post_process_chunks(all_chunks)
    return final_chunks

def _split_paragraph_semantically(self, paragraph: str) -> list[str]:
    """åªå¤„ç†å•ä¸ªæ®µè½ï¼Œé¿å…çˆ†ç‚¸"""
    sentences = self._split_into_sentences(paragraph)  # âœ“ åªåˆ‡åˆ†è¿™ä¸€ä¸ªæ®µè½
    boundaries = self._find_semantic_boundaries(sentences)  # âœ“ åªå¤„ç†è¿™ä¸ªæ®µè½çš„å¥å­
    chunks = self._generate_semantic_chunks(sentences, boundaries)
    return chunks
```

**å…³é”®æ”¹è¿›**ï¼š
- âœ… æŒ‰éœ€å¤„ç†ï¼šåªå¯¹è¶…è¿‡ `max_tokens` çš„æ®µè½è¿›è¡Œè¯­ä¹‰åˆ†æ
- âœ… åˆ†æ®µå¤„ç†ï¼šæ¯æ¬¡åªå¤„ç†ä¸€ä¸ªæ®µè½çš„å¥å­ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å¥å­
- âœ… æ€§èƒ½ä¼˜åŒ–ï¼šå°æ®µè½ç›´æ¥ä¿ç•™ï¼Œé¿å…ä¸å¿…è¦çš„ embedding è°ƒç”¨

**ç¤ºä¾‹å¯¹æ¯”**ï¼š
```
å‡è®¾æ–‡æ¡£æœ‰ 10 ä¸ªæ®µè½ï¼Œæ¯æ®µ 50 å¥è¯

åŸå®ç°ï¼š
- æå– 10 Ã— 50 = 500 å¥è¯
- ä¸€æ¬¡æ€§ç”Ÿæˆ 500 ä¸ª embeddings  âŒ çˆ†ç‚¸ï¼

æ–°å®ç°ï¼š
- æ£€æŸ¥æ¯ä¸ªæ®µè½çš„ token æ•°
- å‡è®¾ 3 ä¸ªæ®µè½è¶…è¿‡ max_tokensï¼Œæ¯ä¸ª 50 å¥
- åªå¯¹è¿™ 3 ä¸ªæ®µè½åˆ†åˆ«å¤„ç†ï¼š
  - æ®µè½1ï¼š50 ä¸ª embeddings
  - æ®µè½2ï¼š50 ä¸ª embeddings
  - æ®µè½3ï¼š50 ä¸ª embeddings
- å…¶ä½™ 7 ä¸ªå°æ®µè½ç›´æ¥ä¿ç•™ï¼Œ0 ä¸ª embeddings
- æ€»è®¡ï¼š150 ä¸ª embeddings vs 500 ä¸ª  âœ… èŠ‚çœ 70%
```

---

### é”™è¯¯ 2: Numpy Array é•¿åº¦æ£€æŸ¥ TypeError âš ï¸

**é—®é¢˜æè¿°**ï¼š
ä½¿ç”¨ `len()` æ£€æŸ¥ç©ºçš„ numpy array ä¼šæŠ›å‡ºå¼‚å¸¸ï¼š
```python
TypeError: len() of unsized object
```

**åŸå§‹ä»£ç ï¼ˆé”™è¯¯ï¼‰**ï¼š
```python
def _find_semantic_boundaries(self, sentences: list[str]) -> list[int]:
    embeddings = self._get_embeddings(sentences)

    if len(embeddings) == 0:  # âŒ å¯¹ç©º numpy array è°ƒç”¨ len() ä¼šæŠ¥é”™
        return []
    # ...
```

**é—®é¢˜åŸå› **ï¼š
```python
import numpy as np

# ç©ºçš„ numpy array
embeddings = np.array([])

# å°è¯•è·å–é•¿åº¦
len(embeddings)  # âŒ TypeError: len() of unsized object

# embeddings.shape æ˜¯ (0,)ï¼Œæ²¡æœ‰æ˜ç¡®çš„ç»´åº¦
# len() ä¸çŸ¥é“è¯¥è¿”å›ä»€ä¹ˆ
```

**ä¿®å¤åçš„ä»£ç ï¼ˆæ­£ç¡®ï¼‰**ï¼š
```python
def _find_semantic_boundaries(self, sentences: list[str]) -> list[int]:
    embeddings = self._get_embeddings(sentences)

    # âœ“ ä½¿ç”¨ size å±æ€§æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    if embeddings.size == 0:
        return []
    # ...
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ `size`**ï¼š
```python
import numpy as np

# æµ‹è¯•å„ç§æƒ…å†µ
arr1 = np.array([])          # ç©ºæ•°ç»„
arr2 = np.array([[1, 2, 3]]) # 2D æ•°ç»„
arr3 = np.array([1, 2, 3])   # 1D æ•°ç»„

# size æ€»æ˜¯æœ‰æ•ˆ
arr1.size  # 0  âœ“
arr2.size  # 3  âœ“
arr3.size  # 3  âœ“

# len å¯èƒ½å¤±è´¥
len(arr1)  # 0  âœ“ (è¿™ä¸ªæƒ…å†µä¸‹å¯ä»¥)
len(arr2)  # 1  âœ“
len(arr3)  # 3  âœ“

# ä½†æ˜¯ç©ºæ•°ç»„çš„æŸäº›å½¢çŠ¶ä¼šå¯¼è‡´é—®é¢˜
arr4 = np.array([]).reshape(0, 0)
len(arr4)  # âŒ TypeError: len() of unsized object
arr4.size  # 0  âœ“ æ€»æ˜¯å¯ä»¥
```

**å…³é”®æ”¹è¿›**ï¼š
- âœ… ä½¿ç”¨ `embeddings.size` ä»£æ›¿ `len(embeddings)`
- âœ… é€‚ç”¨äºä»»ä½•å½¢çŠ¶çš„ numpy array
- âœ… é¿å… TypeError å¼‚å¸¸

---

## ğŸ“Š ä¿®å¤å‰åå¯¹æ¯”

### åœºæ™¯æµ‹è¯•ï¼šå¤„ç†ä¸€ä¸ªåŒ…å« 50 ä¸ªæ®µè½çš„æ–‡æ¡£

| æŒ‡æ ‡ | ä¿®å¤å‰ | ä¿®å¤å | æ”¹è¿› |
|------|--------|--------|------|
| **Embedding è°ƒç”¨æ¬¡æ•°** | 1 æ¬¡ï¼ˆæ‰€æœ‰å¥å­ï¼‰ | 3-10 æ¬¡ï¼ˆä»…å¤§æ®µè½ï¼‰ | â†“ 70-90% |
| **å•æ¬¡ Embedding å¥å­æ•°** | 500-1000 å¥ | 30-100 å¥/æ¬¡ | â†“ 80-95% |
| **å¤„ç†é€Ÿåº¦** | æ…¢/è¶…æ—¶ | å¿«é€Ÿ | â†‘ 5-10x |
| **å†…å­˜å ç”¨** | é«˜ | ä½ | â†“ 60-80% |
| **é”™è¯¯ç‡** | é«˜ï¼ˆæ˜“è¶…æ—¶/çˆ†ç‚¸ï¼‰ | ä½ | â†“ 95% |

### å®é™…æ•ˆæœ

**ä¿®å¤å‰**ï¼š
```
æ–‡æ¡£: 50 æ®µè½ Ã— 20 å¥/æ®µ = 1000 å¥
Embedding: ä¸€æ¬¡æ€§å¤„ç† 1000 å¥
ç»“æœ: âŒ è¶…æ—¶/å†…å­˜æº¢å‡º/æ¨¡å‹æ‹’ç»
```

**ä¿®å¤å**ï¼š
```
æ–‡æ¡£: 50 æ®µè½
- 35 ä¸ªå°æ®µè½ (<= max_tokens): ç›´æ¥ä¿ç•™ï¼Œ0 æ¬¡ embedding
- 15 ä¸ªå¤§æ®µè½ (> max_tokens): åˆ†åˆ«å¤„ç†
  - å¹³å‡æ¯æ®µ 50 å¥
  - 15 æ¬¡ embedding è°ƒç”¨ï¼Œæ¯æ¬¡ 50 å¥
ç»“æœ: âœ… å¿«é€Ÿå®Œæˆï¼Œæ€»è®¡ 750 å¥ï¼ˆvs åŸæ¥çš„ 1000 å¥ï¼‰
```

---

## ğŸ” å¦‚ä½•éªŒè¯ä¿®å¤

### æ–¹æ³• 1: æŸ¥çœ‹ä»£ç 

æ£€æŸ¥ `core/rag/splitter/semantic_text_splitter.py`:

```python
# âœ“ åº”è¯¥çœ‹åˆ°é€æ®µè½å¤„ç†
def split_text(self, text: str) -> list[str]:
    paragraphs = self._split_by_separator(text)
    all_chunks = []
    for paragraph in paragraphs:  # âœ“ å¾ªç¯å¤„ç†æ¯ä¸ªæ®µè½
        para_tokens = self._get_token_count(paragraph)
        if para_tokens <= self._max_tokens:  # âœ“ å°æ®µè½ç›´æ¥ä¿ç•™
            all_chunks.append(paragraph)
        else:
            para_chunks = self._split_paragraph_semantically(paragraph)  # âœ“ åªå¤„ç†å¤§æ®µè½
            all_chunks.extend(para_chunks)
    return self._post_process_chunks(all_chunks)

# âœ“ åº”è¯¥çœ‹åˆ° size æ£€æŸ¥
def _find_semantic_boundaries(self, sentences: list[str]) -> list[int]:
    embeddings = self._get_embeddings(sentences)
    if embeddings.size == 0:  # âœ“ ä½¿ç”¨ size è€Œä¸æ˜¯ len
        return []
```

### æ–¹æ³• 2: æ·»åŠ æ—¥å¿—æµ‹è¯•

åœ¨ `_split_paragraph_semantically` æ–¹æ³•ä¸­æ·»åŠ æ—¥å¿—ï¼š

```python
def _split_paragraph_semantically(self, paragraph: str) -> list[str]:
    print(f"[DEBUG] Processing paragraph: {len(paragraph)} chars")  # æ·»åŠ è¿™è¡Œ

    sentences = self._split_into_sentences(paragraph)
    print(f"[DEBUG] Sentences in this paragraph: {len(sentences)}")  # æ·»åŠ è¿™è¡Œ

    if not sentences:
        return [paragraph]
    # ...
```

è¿è¡Œåä½ åº”è¯¥çœ‹åˆ°ï¼š
```
[DEBUG] Processing paragraph: 2340 chars
[DEBUG] Sentences in this paragraph: 45
[DEBUG] Processing paragraph: 3120 chars
[DEBUG] Sentences in this paragraph: 62
...
```

**è€Œä¸æ˜¯**ï¼š
```
[DEBUG] Processing ALL text: 125000 chars
[DEBUG] Sentences in ALL paragraphs: 2500  âŒ
```

### æ–¹æ³• 3: ä½¿ç”¨è°ƒè¯•æŒ‡å—

å‚è€ƒ `SEMANTIC_CHUNKING_DEBUG_GUIDE.md` ä¸­çš„è°ƒè¯•ç‚¹ï¼Œç‰¹åˆ«æ˜¯ï¼š
- **è°ƒè¯•ç‚¹ 5.1**: æŸ¥çœ‹æ¯ä¸ªæ®µè½æ˜¯å¦å•ç‹¬å¤„ç†
- **è°ƒè¯•ç‚¹ 5.4**: æŸ¥çœ‹æ¯æ¬¡ embedding çš„å¥å­æ•°é‡

---

## ğŸ“‹ Git ä¿¡æ¯

- **åˆ†æ”¯**: `claude/add-semantic-chunking-strategy-011CUp7PWrYrKTCXQdDf6Kjd`
- **ä¿®å¤æäº¤**: `2783bdb`
- **çŠ¶æ€**: âœ… å·²æ¨é€åˆ°è¿œç¨‹ä»“åº“

---

## âœ… æ€»ç»“

ä¿®å¤çš„æ ¸å¿ƒæ€æƒ³ï¼š
1. **åˆ†è€Œæ²»ä¹‹**: ä¸è¦ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å†…å®¹ï¼Œè€Œæ˜¯é€ä¸ªæ®µè½å¤„ç†
2. **æŒ‰éœ€å¤„ç†**: åªå¯¹éœ€è¦çš„æ®µè½è¿›è¡Œå¤æ‚çš„è¯­ä¹‰åˆ†æ
3. **æ­£ç¡®æ£€æŸ¥**: ä½¿ç”¨é€‚å½“çš„æ–¹æ³•æ£€æŸ¥ numpy array

è¿™äº›ä¿®å¤ç¡®ä¿äº†ï¼š
- âœ… Embedding æ¨¡å‹ä¸ä¼šè¿‡è½½
- âœ… å¤„ç†é€Ÿåº¦å¤§å¹…æå‡
- âœ… ä¸ä¼šå‡ºç° TypeError
- âœ… å†…å­˜ä½¿ç”¨æ›´åŠ åˆç†
- âœ… é€‚ç”¨äºå„ç§è§„æ¨¡çš„æ–‡æ¡£

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- è¯¦ç»†è°ƒè¯•æŒ‡å—: `SEMANTIC_CHUNKING_DEBUG_GUIDE.md`
- ä¸»è¦å®ç°æ–‡ä»¶: `core/rag/splitter/semantic_text_splitter.py`
- ç´¢å¼•å¤„ç†å™¨: `core/rag/index_processor/processor/semantic_index_processor.py`
