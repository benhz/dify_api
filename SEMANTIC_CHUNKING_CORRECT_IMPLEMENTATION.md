# è¯­ä¹‰åˆ†å—æ­£ç¡®å®ç°è¯´æ˜

## âœ… å·²å®Œå…¨æŒ‰ç…§è¦æ±‚å®ç°

### æ­£ç¡®çš„ 5 æ­¥æµç¨‹

```
æ–‡æœ¬æµï¼š[æ®µè½1...æ®µè½2...æ®µè½3...æ®µè½4...]
    â†“
Step 1: separator åˆ‡åˆ†ç‰©ç†è¾¹ç•Œ
    [æ®µè½1] | [æ®µè½2] | [æ®µè½3] | [æ®µè½4]
    â†“
Step 2: è¯­ä¹‰åˆ†æ + threshold_amount/buffer_size
    åœ¨æ¯ä¸ªæ®µè½å†…æ‰¾è¯­ä¹‰æ–­ç‚¹
    [æ®µè½1-å¥1,2] | [æ®µè½1-å¥3,4] | [æ®µè½2-å¥1,2] | ...
    â†“
Step 3: max_tokens å¼ºåˆ¶åˆ‡æ–­è¿‡é•¿çš„å—
    [å—1: 1000 tokens] | [å—2: 950 tokens] | ...
    â†“
Step 4: chunk_overlap æ·»åŠ é‡å 
    [å—1]
    [å—1æœ«å°¾50 + å—2 + ...]
    [å—2æœ«å°¾50 + å—3 + ...]
    â†“
Step 5: min_chunk_tokens / max_chunk_tokens
    ç¡®ä¿æ¯ä¸ªå—åœ¨æœ€å°å’Œæœ€å¤§ tokens èŒƒå›´å†…
```

---

## ğŸ“ å®ç°ç»†èŠ‚

### Step 1: separator - ç‰©ç†è¾¹ç•Œåˆ‡åˆ†
**æ–¹æ³•**: `_split_by_separator(text)`
**è¾“å…¥**: å®Œæ•´æ–‡æ¡£æ–‡æœ¬
**è¾“å‡º**: æ®µè½åˆ—è¡¨
**é€»è¾‘**:
```python
paragraphs = text.split(separator)  # ä¾‹å¦‚ "\n\n"
return [p.strip() for p in paragraphs if p.strip()]
```

---

### Step 2: è¯­ä¹‰åˆ†æ
**æ–¹æ³•**: `_apply_semantic_splitting(paragraph)`
**è¾“å…¥**: å•ä¸ªæ®µè½
**è¾“å‡º**: è¯­ä¹‰å—åˆ—è¡¨
**é€»è¾‘**:
1. åˆ‡åˆ†å¥å­: `sentences = self._split_into_sentences(paragraph)`
2. ç”Ÿæˆ embeddings: `embeddings = self._get_embeddings(sentences)`
3. è®¡ç®—ç›¸ä¼¼åº¦: è®¡ç®—ç›¸é‚»å¥å­é—´çš„ cosine ç›¸ä¼¼åº¦
4. å¹³æ»‘å¤„ç†: ä½¿ç”¨ `buffer_size` è¿›è¡Œç§»åŠ¨å¹³å‡
5. é˜ˆå€¼åˆ¤æ–­: ä½¿ç”¨ `threshold_amount` percentile æ‰¾è¾¹ç•Œ
6. ç”Ÿæˆå—: æŒ‰è¾¹ç•Œç»„åˆå¥å­

**å…³é”®å‚æ•°**:
- `threshold_amount`: 95 (ç™¾åˆ†ä½æ•°ï¼Œè¶Šé«˜è¾¹ç•Œè¶Šå°‘)
- `buffer_size`: 2 (å¹³æ»‘çª—å£å¤§å°)

**é‡è¦**: å¯¹**æ¯ä¸ªæ®µè½**éƒ½è¿›è¡Œè¯­ä¹‰åˆ†æï¼Œä¸æ˜¯åªå¯¹é•¿æ®µè½

---

### Step 3: max_tokens å¼ºåˆ¶åˆ‡æ–­
**æ–¹æ³•**: `_enforce_max_tokens(chunks)`
**è¾“å…¥**: Step 2 çš„è¯­ä¹‰å—åˆ—è¡¨
**è¾“å‡º**: æ‰€æœ‰å—éƒ½ â‰¤ max_tokens çš„å—åˆ—è¡¨
**é€»è¾‘**:
```python
for chunk in chunks:
    if token_count(chunk) > max_tokens:
        # æŒ‰å¥å­åˆ‡åˆ†
        sentences = split_into_sentences(chunk)
        # é‡æ–°ç»„åˆï¼Œç¡®ä¿ä¸è¶…è¿‡ max_tokens
        # å¦‚æœå•å¥è¶…è¿‡ max_tokensï¼Œå¼ºåˆ¶æŒ‰è¯åˆ‡åˆ†
```

---

### Step 4: chunk_overlap æ·»åŠ é‡å 
**æ–¹æ³•**: `_add_overlap(chunks)`
**è¾“å…¥**: Step 3 çš„å—åˆ—è¡¨
**è¾“å‡º**: æ·»åŠ äº†é‡å çš„å—åˆ—è¡¨
**é€»è¾‘**:
```python
overlapped_chunks = []
for i, chunk in enumerate(chunks):
    if i > 0:
        # ä»å‰ä¸€ä¸ªå—è·å–æœ€å N ä¸ª tokens
        prefix = get_last_n_tokens(chunks[i-1], chunk_overlap)
        chunk = prefix + ' ' + chunk
    overlapped_chunks.append(chunk)
```

**ç»“æ„ç¤ºä¾‹**:
```
åŸå§‹:    [AAAA] [BBBB] [CCCC]
overlap: [AAAA] [aaAABBBB] [bbBBCCCC]
         â””â”€50â”€â”˜ â””â”€50â”˜        â””â”€50â”˜
```

---

### Step 5: min/max tokens çº¦æŸ
**æ–¹æ³•**: `_enforce_size_constraints(chunks)`
**è¾“å…¥**: Step 4 çš„å—åˆ—è¡¨
**è¾“å‡º**: æœ€ç»ˆç¬¦åˆå¤§å°çº¦æŸçš„å—åˆ—è¡¨
**é€»è¾‘**:
1. **åˆå¹¶çŸ­å—**:
   ```python
   if token_count(chunk) < min_chunk_tokens:
       # å°è¯•ä¸å‰å—åˆå¹¶
       # æˆ–ä¸åå—åˆå¹¶
   ```

2. **åˆ‡åˆ†é•¿å—**:
   ```python
   if token_count(chunk) > max_chunk_tokens:
       # æŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†
       # ç¡®ä¿æ¯å— <= max_chunk_tokens
   ```

---

## ğŸ” å…³é”®æ”¹è¿›

### 1. æ‰€æœ‰æ®µè½éƒ½è¿›è¡Œè¯­ä¹‰åˆ†æ
```python
# âŒ é”™è¯¯çš„æ—§å®ç°
for paragraph in paragraphs:
    if token_count(paragraph) > max_tokens:  # åªå¤„ç†é•¿æ®µè½
        apply_semantic_splitting(paragraph)
    else:
        chunks.append(paragraph)  # çŸ­æ®µè½ç›´æ¥åŠ å…¥

# âœ“ æ­£ç¡®çš„æ–°å®ç°
for paragraph in paragraphs:
    # æ¯ä¸ªæ®µè½éƒ½è¿›è¡Œè¯­ä¹‰åˆ†æ
    semantic_chunks = apply_semantic_splitting(paragraph)
    all_chunks.extend(semantic_chunks)
```

### 2. æ­£ç¡®çš„å¤„ç†é¡ºåº
```python
# âœ“ æ–°å®ç°ä¸¥æ ¼æŒ‰ç…§ 5 æ­¥é¡ºåº
paragraphs = step1_split_by_separator(text)
semantic_chunks = step2_semantic_analysis(paragraphs)
limited_chunks = step3_enforce_max_tokens(semantic_chunks)
overlapped_chunks = step4_add_overlap(limited_chunks)
final_chunks = step5_enforce_size_constraints(overlapped_chunks)
```

### 3. è¿”å›æ­£ç¡®çš„é¢„è§ˆå†…å®¹
```python
# SemanticIndexProcessor.transform() è¿”å›:
return all_documents  # List[Document]

# æ¯ä¸ª Document:
Document(
    page_content="åˆ†å—åçš„æ–‡æœ¬å†…å®¹",  # ç”¨äºé¢„è§ˆ
    metadata={
        "doc_id": "uuid",
        "doc_hash": "hash"
    }
)
```

---

## ğŸ“Š å‚æ•°è¯´æ˜

### å¿…éœ€å‚æ•°
| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|------|------|--------|
| `separator` | æ®µè½åˆ†éš”ç¬¦ | `"\n\n"` |
| `max_tokens` | ç¡¬æ€§tokenä¸Šé™ | `1024` |

### è¯­ä¹‰åˆ†æå‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | èŒƒå›´ |
|------|------|--------|------|
| `threshold_amount` | ç›¸ä¼¼åº¦é˜ˆå€¼ç™¾åˆ†ä½ | `95` | 0-100 |
| `buffer_size` | å¹³æ»‘çª—å£å¤§å° | `2` | â‰¥0 |

### åå¤„ç†å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `chunk_overlap` | é‡å tokenæ•° | `50` |
| `min_chunk_tokens` | æœ€å°å—å¤§å° | `150` |
| `max_chunk_tokens` | æœ€å¤§å—å¤§å° | `max_tokens` |

---

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### API è¯·æ±‚
```json
{
  "doc_form": "semantic_model",
  "process_rule": {
    "mode": "custom",
    "rules": {
      "segmentation": {
        "separator": "\\n\\n",
        "max_tokens": 1024,
        "chunk_overlap": 50,
        "threshold_amount": 95,
        "buffer_size": 2,
        "min_chunk_tokens": 150,
        "max_chunk_tokens": 1000
      }
    }
  }
}
```

### å¤„ç†ç¤ºä¾‹
**è¾“å…¥æ–‡æœ¬**:
```
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œã€‚

è®¡ç®—æœºè§†è§‰æ˜¯æ·±åº¦å­¦ä¹ çš„é‡è¦åº”ç”¨ã€‚å·ç§¯ç¥ç»ç½‘ç»œè¡¨ç°å‡ºè‰²ã€‚
```

**Step 1: separator åˆ‡åˆ†**:
```
[æ®µè½1: "æ·±åº¦å­¦ä¹ ...ç¥ç»ç½‘ç»œã€‚"]
[æ®µè½2: "è®¡ç®—æœºè§†è§‰...è¡¨ç°å‡ºè‰²ã€‚"]
```

**Step 2: è¯­ä¹‰åˆ†æ**:
```
æ®µè½1 â†’ [å—1: "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚"]
        [å—2: "å®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œã€‚"]
æ®µè½2 â†’ [å—3: "è®¡ç®—æœºè§†è§‰æ˜¯æ·±åº¦å­¦ä¹ çš„é‡è¦åº”ç”¨ã€‚"]
        [å—4: "å·ç§¯ç¥ç»ç½‘ç»œè¡¨ç°å‡ºè‰²ã€‚"]
```

**Step 3: max_tokens**:
```
(å‡è®¾éƒ½ â‰¤ max_tokensï¼Œä¿æŒä¸å˜)
```

**Step 4: chunk_overlap**:
```
[å—1]
[å—1æœ«å°¾ + å—2]
[å—2æœ«å°¾ + å—3]
[å—3æœ«å°¾ + å—4]
```

**Step 5: min/max çº¦æŸ**:
```
(å‡è®¾éƒ½åœ¨èŒƒå›´å†…ï¼Œä¿æŒä¸å˜)
```

---

## ğŸ“ ä»£ç ä½ç½®

### æ ¸å¿ƒæ–‡ä»¶
- **æ–‡æœ¬åˆ†å‰²å™¨**: `core/rag/splitter/semantic_text_splitter.py`
  - `split_text()` - ä¸»å…¥å£ï¼Œæ‰§è¡Œ 5 æ­¥æµç¨‹
  - `_apply_semantic_splitting()` - Step 2
  - `_enforce_max_tokens()` - Step 3
  - `_add_overlap()` - Step 4
  - `_enforce_size_constraints()` - Step 5

- **ç´¢å¼•å¤„ç†å™¨**: `core/rag/index_processor/processor/semantic_index_processor.py`
  - `transform()` - è¿”å› Document å¯¹è±¡ç”¨äºé¢„è§ˆ

### å…³é”®æ–¹æ³•è°ƒç”¨é“¾
```
API Request
  â†“
DatasetIndexingEstimateApi.post()
  â†“
DocumentService.estimate()
  â†“
IndexingRunner.indexing_estimate()
  â†“
SemanticIndexProcessor.transform()
  â†“
SemanticTextSplitter.split_text()
  â†“
5 æ­¥å¤„ç†æµç¨‹
  â†“
è¿”å› Document å¯¹è±¡åˆ—è¡¨ (ç”¨äºé¢„è§ˆ)
```

---

## âœ… éªŒè¯æ¸…å•

- [x] Step 1: separator æ­£ç¡®åˆ‡åˆ†æ®µè½
- [x] Step 2: æ¯ä¸ªæ®µè½éƒ½è¿›è¡Œè¯­ä¹‰åˆ†æï¼ˆä¸æ˜¯åªå¤„ç†é•¿æ®µè½ï¼‰
- [x] Step 3: max_tokens å¼ºåˆ¶åˆ‡æ–­é•¿å—
- [x] Step 4: chunk_overlap æ­£ç¡®æ·»åŠ é‡å 
- [x] Step 5: min/max tokens çº¦æŸæ­£ç¡®æ‰§è¡Œ
- [x] è¿”å› Document å¯¹è±¡ç”¨äºé¢„è§ˆ
- [x] é¿å… embedding çˆ†ç‚¸ï¼ˆæ¯æ®µè½å•ç‹¬å¤„ç†ï¼‰
- [x] æ—  TypeError (ä½¿ç”¨ embeddings.size)

---

## ğŸ”— Git ä¿¡æ¯

- **åˆ†æ”¯**: `claude/add-semantic-chunking-strategy-011CUp7PWrYrKTCXQdDf6Kjd`
- **æäº¤**: `cf9d7c4`
- **çŠ¶æ€**: âœ… å·²æ¨é€

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- è°ƒè¯•æŒ‡å—: `SEMANTIC_CHUNKING_DEBUG_GUIDE.md`
- ä¿®å¤è¯´æ˜: `SEMANTIC_CHUNKING_FIXES.md`
- æœ¬æ–‡æ¡£: `SEMANTIC_CHUNKING_CORRECT_IMPLEMENTATION.md`
